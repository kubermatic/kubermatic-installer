# THIS FILE IS GENERATED BY https://github.com/kubermatic/kubermatic/blob/master/api/hack/sync-charts.sh
# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  # The prefix for monitoring annotations in the user cluster. Default: monitoring.kubermatic.io -> monitoring.kubermatic.io/scrape, monitoring.kubermatic.io/path
  monitoringScrapeAnnotationPrefix: ""
  deployVPA: true

  # helm hooks/checks
  checks:
    # Checks if the last release contains the kubermatic CRD's.
    # We moved them out of the chart to avoid issues with helm
    crd:
      disable: false
      helmVersion: "v2.11.0"

  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"
  controller:
    # Available feature gates:
    # - OpenIDAuthPlugin
    #   If enabled configures the flags on the API server to use OAuth2 identity providers.
    # - VerticalPodAutoscaler
    #   If enabled the cluster-controller will enable the VerticalPodAutoscaler for all control plane components
    # For example:
    # featureGates: "OpenIDAuthPlugin=true,VerticalPodAutoscaler=true"
    featureGates: ""
    datacenterName: ""
    # Specifies the NodePort range for customer clusters - this must match the NodePort range of the seed cluster.
    nodeportRange: "30000-32767"
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.9.0"
      pullPolicy: "IfNotPresent"
    addons:
      image:
        repository: "quay.io/kubermatic/addons"
        tag: "v0.1.16"
        pullPolicy: "IfNotPresent"
      # list of Addons to install into every user-cluster. All need to exist in the addons image
      defaultAddons:
      - canal
      - dashboard
      - dns
      - kube-proxy
      - openvpn
      - rbac
      - kubelet-configmap
      - default-storage-class
  api:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.9.0"
      pullPolicy: "IfNotPresent"
  ui:
    replicas: 2
    image:
      repository: "quay.io/kubermatic/ui-v2"
      tag: "v1.1.0"
      pullPolicy: "IfNotPresent"
    config: |
      {
        "share_kubeconfig": false,
        "show_demo_info": false,
        "show_terms_of_service": false,
        "cleanup_cluster": false
      }
  rbac:
    replicas: 1
    image:
      repository: "quay.io/kubermatic/api"
      tag: "v2.9.0"
      pullPolicy: "IfNotPresent"
  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup
  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning
  clusterNamespacePrometheus: {}
  vpa:
    updater:
      image:
        repository: k8s.gcr.io/vpa-updater
        tag: 0.3.0
    recommender:
      image:
        repository: k8s.gcr.io/vpa-recommender
        tag: 0.3.0
    admissioncontroller:
      image:
        repository: k8s.gcr.io/vpa-admission-controller
        tag: 0.3.0

# ====== cert-manager ======
certManager:
  image:
    repository: quay.io/jetstack/cert-manager-controller
    tag: v0.6.0
    pullPolicy: IfNotPresent

  webhookImage:
    repository: quay.io/jetstack/cert-manager-webhook
    tag: v0.6.0
    pullPolicy: IfNotPresent

  caSyncImage:
    repository: quay.io/munnerz/apiextensions-ca-helper
    tag: v0.1.0
    pullPolicy: IfNotPresent

  securityContext:
    enabled: false
    fsGroup: 1001
    runAsUser: 1001

  ingressShim: {}
    # defaultIssuerName: ""
    # defaultIssuerKind: ""
    # defaultACMEChallengeType: ""
    # defaultACMEDNS01ChallengeProvider: ""

# ====== certs ======
certificates:
  domains:
  - "kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "kibana.kubermatic.example.com"
  issuer:
    email: dev@loodse.com

# ====== nginx-ingress-controller ======
##### Kubermatic ingress
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  ignoreMasterTaint: false
  prometheus:
    port: 10254
    scrape: true
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.22.0
  config: {}
#    load-balance: "least_conn"

# ====== nodeport-proxy ======
nodePortPoxy:
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "v1.2"

# ====== oauth ======
##### Kubermatic oauth
dex:
  image:
    repository: "quay.io/dexidp/dex"
    tag: "v2.12.0"
  replicas: 1
  ingress:
    host: ""
    path: "/dex"
#  connectors:
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: some-client-id
#      clientSecret: some-client-secret
#      redirectURI: https://dev.kubermatic.io/dex/callback
#      orgs:
#      - name: kubermatic
#
#  clients:
#  - id: kubermatic
#    name: Kubermatic
#    secret: very-secret
#    RedirectURIs:
#    - http://localhost:8000
#    - https://dev.kubermatic.io
#    - http://localhost:8000/projects
#    - https://dev.kubermatic.io/projects
#
#  staticPasswordLogins:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"

# ====== minio ======
minio:
  image:
    repository: "minio/minio"
    tag: "RELEASE.2019-01-16T21-44-08Z"
  storeSize: "100Gi"
  credentials:
    accessKey: "wtupllWfpMg414ZM5YkzZiUmgjh1vZdk"
    secretKey: "r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym"
  backups: true

  # If your cluster does not have a default storage class,
  # you can specify the class to use for Minio. Note that
  # you cannot change this later on without purging the
  # chart and losing data.
  #storageClass: hdd

# ====== iap ======
iap:
  deployments:
    # alertmanager:
    #   name: alertmanager
    #   client_id: alertmanager
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   ## example configuration allowing access only to the mygroup from mygithuborg organization
    #     scopes:
    #     - "groups"
    #     resources:
    #     - uri: "/*"
    #       groups:
    #       - "mygithuborg:mygroup"
    #   upstream_service: alertmanager-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9093
    #   ingress:
    #     host: "alertmanager.kubermatic.tld"
    #     annotations: {}
    # grafana:
    #   name: grafana
    #   client_id: grafana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: grafana.monitoring.svc.cluster.local
    #   upstream_port: 3000
    #   ingress:
    #     host: "grafana.kubermatic.tld"
    #     annotations: {}
    # kibana:
    #   name: kibana
    #   client_id: kibana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: kibana.logging.svc.cluster.local
    #   upstream_port: 5601
    #   ingress:
    #     host: "kibana.kubermatic.tld"
    #     annotations: {}
    # prometheus:
    #   name: prometheus
    #   client_id: prometheus
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: prometheus-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9090
    #   ingress:
    #     host: "prometheus.kubermatic.tld"
    #     annotations:
    #       ingress.kubernetes.io/upstream-hash-by: "ip_hash" ## needed for prometheus federations

  discovery_url: https://kubermatic.tld/dex/.well-known/openid-configuration
  port: 3000

  image:
    repository: quay.io/gambol99/keycloak-proxy
    tag: v2.3.0
    pullPolicy: IfNotPresent

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 10m
    #   memory: 64Mi


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  version: v0.16.0
  host: ""
  config:
    global:
      slack_api_url: https://
    route:
      receiver: 'default'
      repeat_interval: 1h
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true

# ====== grafana ======
grafana:
  user: YWRtaW4= # admin
  password: bG9vZHNlMTIz # loodse123
  image:
    repository: grafana/grafana
    tag: 5.4.3

  # Control where the provisioning files are located.
  # If you want to *not* use the predefined ones, this
  # allows you to specify your own directory without
  # having to touch the existing files. If you only want
  # to add new elements, you can just place your YAML files
  # into the directories.
  provisioning:
    dashboards:
      source: provisioning/dashboards/*

      # You can specify additional dashboard sources inline as well.
      #extra:
      #- folder: "Initech Resources"
      #  name: "initech"
      #  options:
      #    path: /grafana-dashboard-definitions/initech
      #  org_id: 1
      #  type: file

    datasources:
      source: provisioning/datasources/*

      # You can specify additional datasources inline as well.
      #extra:
      #- name: influxdb
      #  type: influxdb
      #  access: proxy
      #  org_id: 1
      #  url: http://influxdb.monitoring.svc.cluster.local:9090
      #  version: 1
      #  editable: false

    # override Grafana configuration flags
    configuration:
      # change this to "Editor" to allow OAuth-authenticated users
      # to add and edit the dashboards
      auto_assign_org_role: Viewer

  # If you manage your dashboards via your own configmaps,
  # you can add them here to have them automatically be
  # mounted in Grafana. For each volume, specify either a
  # configMap name or a secretName, never both.
  #volumes:
  #- name: initech-public-dashboards
  #  mountPath: /initech/public-dashboards
  #  configMap: initech-dashboards-configmap
  #- name: initech-secret-dashboards
  #  mountPath: /initech/secret-dashboards
  #  secretName: initech-dashboards-secret

  # These values are injected into the dashboard JSON files,
  # see the README.md for more information on how templating
  # works.
  dashboards:
    editable: false
    transparentPanels: true
    refresh: 30s
    defaultRange: now-1h

    # Filter the namespaces shown in the Kubernetes/Volume
    # dashboard, e.g. to not show customer clusters by setting
    #
    #     namespace!~'cluster-.*'
    #
    # Do not use double quotes inside this value.
    volumeDashboardNamespaceFilter: ""

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.5.0
  resizer:
    image:
      repository: k8s.gcr.io/addon-resizer
      tag: '1.7'

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.17.0

# ====== prometheus ======
prometheus:
  # admin:loodse123
  auth: 'YWRtaW46JGFwcjEkUFR0Y3lQc0MkWVZFTk9nZmpiVE52RWtSeVk1ZjJULgo='
  version: 'v2.6.0'
  host: ""
  storageSize: 100Gi
  backups: true

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - 'config/scraping/*.yaml'
    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - 'config/alertmanagers/*.yaml'
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - '/etc/prometheus/rules/*.yaml'

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.2/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  # For larger deployments it can make sense to increase the CPU/memory
  # limits accordingly.
  containers:
    prometheus:
      resources:
        limits:
          cpu: 1
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 512Mi
    reloader:
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 25m
          memory: 16Mi


# =======================
# ======= Logging =======
# =======================

# ====== elasticsearch ======
logging:
  elasticsearch:
    dataReplicas: 5
    masterReplicas: 3
    storageSize: 10Gi
    image:
      repository: docker.elastic.co/elasticsearch/elasticsearch
      tag: "6.5.1"
      pullPolicy: IfNotPresent
    curator:
      # Amount of days after which the indicies should be killed
      interval: 5
      image:
        repository: quay.io/kubermatic/elasticsearch-curator
        tag: "5.6.0-1"
        pullPolicy: IfNotPresent
    init:
      image:
        repository: docker.io/busybox
        tag: "1.27.2"
        pullPolicy: IfNotPresent

# ====== kibana ======
logging:
  kibana:
    image:
      repository: docker.elastic.co/kibana/kibana
      tag: "6.5.1"
      pullPolicy: "IfNotPresent"

# ====== fluentbit ======
logging:
  fluentbit:
    image:
      repository: fluent/fluent-bit
      tag: 0.14.7
      pullPolicy: "IfNotPresent"


# =======================
# ======= Backups =======
# =======================

# ====== ark ======
ark:
  # the Docker image for Ark;
  # if you are using restic, make sure to use an official image
  # that also contains the restic binary
  image:
    repository: gcr.io/heptio-images/ark
    tag: v0.10.0
    pullPolicy: IfNotPresent

  # CLI flags to pass to ark server; note that the two flags
  # `default-backup-storage-location` and `default-volume-snapshot-locations`
  # are automatically set via the ark-config Helm chart's
  # values.
  serverFlags:
    - --metrics-address=:8085
    - --backup-sync-period=1m

  # whether or not to create a restic daemonset
  # restic: true

  # configure the credentials used to make snapshots (when using
  # persistentVolumeProvider) and to store backups; you can enable
  # multiple credentials, if for some reason you run on GCP and
  # still want to make restic snapshots to be stored in AWS S3.
  credentials: {}
    #aws:
    #  accessKey: ...
    #  secretKey: ...
    #gcp:
    #  serviceKey: '{...}'
    #azure:
    #  AZURE_SUBSCRIPTION_ID: ...
    #  AZURE_TENANT_ID: ...
    #  AZURE_RESOURCE_GROUP: ...
    #  AZURE_CLIENT_ID: ...
    #  AZURE_CLIENT_SECRET: ...
    #  AZURE_STORAGE_ACCOUNT_ID: ...
    #  AZURE_STORAGE_KEY: ...
    #restic:
    #  password: averysecurepassword

  # Only kube2iam: change the AWS_ACCOUNT_ID and HEPTIO_ARK_ROLE_NAME
  podAnnotations: {}
  # iam.amazonaws.com/role: arn:aws:iam::<AWS_ACCOUNT_ID>:role/<HEPTIO_ARK_ROLE_NAME>

  tolerations: []
  nodeSelector: {}

# ====== ark-config ======
ark:
  # define one of your backupStorageLocations as the default
  #defaultBackupStorageLocation: aws

  # see https://heptio.github.io/ark/v0.10.0/api-types/backupstoragelocation.html
  #backupStorageLocations:
  #  aws:
  #    provider: aws
  #    objectStorage:
  #      bucket: myclusterbackups
  #    config:
  #      region: eu-west-1

  # optionally define some of your volumeSnapshotLocations as the default;
  # each element in the list must be a string of the form "provider:location"
  #defaultVolumeSnapshotLocations:
  #  - aws:aws

  # see https://heptio.github.io/ark/v0.10.0/api-types/volumesnapshotlocation.html
  #volumeSnapshotLocations:
  #  aws:
  #    provider: aws
  #    config:
  #      region: eu-west-1

  # glob expressions to find schedule defitions
  schedulesPath: schedules/*

