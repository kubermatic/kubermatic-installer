# THIS FILE IS GENERATED BY https://github.com/kubermatic/kubermatic/blob/master/api/hack/sync-charts.sh
# ====== kubermatic ======
kubermatic:
  # Whether the cluster is a master cluster
  # This must be false for any non-master seed
  isMaster: true
  # the base64 encoded docker/quay authentication json file
  imagePullSecretData: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"
  controller:
    datacenterName: ""
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.8.0-rc.3"
      pullPolicy: "IfNotPresent"
    addons:
      image:
        repository: "quay.io/kubermatic/addons"
        tag: "v0.1.12"
        pullPolicy: "IfNotPresent"
      # list of Addons to install into every user-cluster. All need to exist in the addons image
      defaultAddons:
      - canal
      - dashboard
      - dns
      - kube-proxy
      - openvpn
      - rbac
      - kubelet-configmap
      - default-storage-class
      - metrics-server
  api:
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.8.0-rc.3"
      pullPolicy: "IfNotPresent"
  ui:
    replicas: 2
    image:
      repository: "kubermatic/ui-v2"
      tag: "v1.0.0"
      pullPolicy: "IfNotPresent"
    config: |
      {
        "show_demo_info": false
      }
  rbac:
    replicas: 1
    image:
      repository: "kubermatic/api"
      tag: "v2.8.0-rc.3"
      pullPolicy: "IfNotPresent"
  s3_exporter:
    image:
      repository: quay.io/kubermatic/s3-exporter
      tag: v0.2
    endpoint: http://minio.minio.svc.cluster.local:9000
    bucket: kubermatic-etcd-backups
  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup
  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
#  clusterNamespacePrometheus:
#    disableDefaultScrapingConfigs: true
#    scrapingConfigs:
#    - job_name: 'schnitzel'
#      kubernetes_sd_configs:
#      - role: pod
#      relabel_configs:
#      - source_labels: [__meta_kubernetes_pod_annotation_kubermatic_scrape]
#        action: keep
#        regex: true
#    disableDefaultRules: false
#    rules:
#      groups:
#      - name: my-custom-group
#        rules:
#        - alert: MyCustomAlert
#          annotations:
#            message: Something happend in {{ $labels.namespace }}
#          expr: |
#            sum(rate(machine_controller_errors_total[5m])) by (namespace) > 0.01
#          for: 10m
#          labels:
#            severity: warning
  clusterNamespacePrometheus: {}

# ====== cert-manager ======
certManager:
  image:
    repository: quay.io/jetstack/cert-manager-controller
    tag: v0.4.1
    pullPolicy: Always

# ====== certs ======
certificates:
  domains:
  - "kubermatic.example.com"
  - "grafana.kubermatic.example.com"
  - "prometheus.kubermatic.example.com"
  - "alertmanager.kubermatic.example.com"
  - "kibana.kubermatic.example.com"

# ====== nginx-ingress-controller ======
##### Kubermatic ingress
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  ignoreMasterTaint: false
  prometheus:
    port: 10254
    scrape: true
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.18.0
  defaultBackend:
    image:
      repository: gcr.io/google_containers/defaultbackend
      tag: 1.4
  config: {}
#    load-balance: "least_conn"

# ====== nodeport-proxy ======
nodePortPoxy:
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "v1.2"

# ====== oauth ======
##### Kubermatic oauth
dex:
  image:
    repository: "quay.io/dexidp/dex"
    tag: "v2.11.0"
  replicas: 1
  ingress:
    host: ""
    path: "/dex"
#  connectors:
#  - type: github
#    id: github
#    name: GitHub
#    config:
#      clientID: some-client-id
#      clientSecret: some-client-secret
#      redirectURI: https://dev.kubermatic.io/dex/callback
#      orgs:
#      - name: kubermatic
#
#  clients:
#  - id: kubermatic
#    name: Kubermatic
#    secret: very-secret
#    RedirectURIs:
#    - http://localhost:8000
#    - https://dev.kubermatic.io
#    - http://localhost:8000/clusters
#    - https://dev.kubermatic.io/clusters
#
#  staticPasswordLogins:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"

# ====== minio ======
minio:
  image:
    repository: "minio/minio"
    tag: "RELEASE.2018-06-09T03-43-35Z"
  storeSize: "100Gi"
  credentials:
    accessKey: "wtupllWfpMg414ZM5YkzZiUmgjh1vZdk"
    secretKey: "r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym"
  backups: true

# ====== iap ======
iap:
  deployments:
    # alertmanager:
    #   name: alertmanager
    #   client_id: alertmanager
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   ## example configuration allowing access only to the mygroup from mygithuborg organization
    #     scopes:
    #     - "groups"
    #     resources:
    #     - uri: "/*"
    #       groups:
    #       - "mygithuborg:mygroup"
    #   upstream_service: alertmanager-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9093
    #   ingress:
    #     host: "alertmanager.kubermatic.tld"
    #     annotations: {}
    # grafana:
    #   name: grafana
    #   client_id: grafana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: grafana.monitoring.svc.cluster.local
    #   upstream_port: 3000
    #   ingress:
    #     host: "grafana.kubermatic.tld"
    #     annotations: {}
    # kibana:
    #   name: kibana
    #   client_id: kibana
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: kibana.logging.svc.cluster.local
    #   upstream_port: 5601
    #   ingress:
    #     host: "kibana.kubermatic.tld"
    #     annotations: {}
    # prometheus:
    #   name: prometheus
    #   client_id: prometheus
    #   client_secret: xxx
    #   encryption_key: xxx
    #   config: {} ## see https://github.com/gambol99/keycloak-proxy#configuration
    #   upstream_service: prometheus-kubermatic.monitoring.svc.cluster.local
    #   upstream_port: 9090
    #   ingress:
    #     host: "prometheus.kubermatic.tld"
    #     annotations:
    #       ingress.kubernetes.io/upstream-hash-by: "ip_hash" ## needed for prometheus federations

  discovery_url: https://kubermatic.tld/dex/.well-known/openid-configuration
  port: 3000

  image:
    repository: quay.io/gambol99/keycloak-proxy
    tag: v2.3.0
    pullPolicy: IfNotPresent

  resources: {}
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 10m
    #   memory: 64Mi


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  version: v0.15.0
  host: ""
  config:
    global:
      slack_api_url: https://
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster', 'seed_cluster']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true

# ====== grafana ======
grafana:
  user: YWRtaW4= # admin
  password: bG9vZHNlMTIz # loodse123
  image:
    repository: grafana/grafana
    tag: 5.2.1

  # Control where the provisioning files are located.
  # If you want to *not* use the predefined ones, this
  # allows you to specify your own directory without
  # having to touch the existing files. If you only want
  # to add new elements, you can just place your YAML files
  # into the directories.
  provisioning:
    dashboards:
      source: provisioning/dashboards/*

      # You can specify additional dashboard sources inline as well.
      #extra:
      #- folder: "Initech Resources"
      #  name: "initech"
      #  options:
      #    path: /grafana-dashboard-definitions/initech
      #  org_id: 1
      #  type: file

    datasources:
      source: provisioning/datasources/*

      # You can specify additional datasources inline as well.
      #extra:
      #- name: influxdb
      #  type: influxdb
      #  access: proxy
      #  org_id: 1
      #  url: http://influxdb.monitoring.svc.cluster.local:9090
      #  version: 1
      #  editable: false

  # If you manage your dashboards via your own configmaps,
  # you can add them here to have them automatically be
  # mounted in Grafana. For each volume, specify either a
  # configMap name or a secretName, never both.
  #volumes:
  #- name: initech-public-dashboards
  #  mountPath: /initech/public-dashboards
  #  configMap: initech-dashboards-configmap
  #- name: initech-secret-dashboards
  #  mountPath: /initech/secret-dashboards
  #  secretName: initech-dashboards-secret

  # These values are injected into the dashboard JSON files,
  # see the README.md for more information on how templating
  # works.
  dashboards:
    editable: false
    transparentPanels: true
    refresh: 30s
    defaultRange: now-1h

    # Filter the namespaces shown in the Kubernetes/Volume
    # dashboard, e.g. to not show customer clusters by setting
    #
    #     namespace!~'cluster-.*'
    #
    # Do not use double quotes inside this value.
    volumeDashboardNamespaceFilter: ""

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.3.1
  resizer:
    image:
      repository: k8s.gcr.io/addon-resizer
      tag: '1.7'

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.15.2

# ====== prometheus ======
prometheus:
  # admin:loodse123
  auth: 'YWRtaW46JGFwcjEkUFR0Y3lQc0MkWVZFTk9nZmpiVE52RWtSeVk1ZjJULgo='
  version: 'v2.2.1'
  host: ""
  storageSize: 100Gi
  backups: true

  # Specify additional external labels which will be added to all
  # alerts sent by Prometheus.
  #externalLabels:
  #  seed_cluster: default

  # Configure the scraping rules for Prometheus. You can either
  # add your own scraping configs here or change the path to the
  # predefined config files that are evaluated when Helm builds
  # the chart and deploys it. You cannot use this to load files
  # at runtime from a custom volume because Prometheus does not
  # support it.
  scraping:
    files:
    - 'config/scraping/*.yaml'
    #configs:
    #- job_name: myscrapejob
    #  honor_labels: true
    #  ...

  # Similarly to the scraping config, you can configure the
  # target alertmanagers here.
  alertmanagers:
    files:
    - 'config/alertmanagers/*.yaml'
    #configs:
    #- scheme: http
    #  path_prefix: /
    #  ...

  # The list of rule files to load; if you use the `volumes`
  # directive below to mount your own ConfigMap or Secret into
  # Prometheus, you will want to extend this list to laod your
  # own rule files. You can remove the predefined path to
  # effectively disable the stock recordings and alerts.
  ruleFiles:
  - '/etc/prometheus/rules/*.yaml'

  # Optionally add some more recording/alerting rules; the structure
  # beneath `rules` is identical to regular rules files as documented
  # in https://prometheus.io/docs/prometheus/2.2/getting_started/
  # For larger collections of rules, consider using the custom volume
  # approach shown further down in the `volumes` section.
  #rules:
  #  groups:
  #    - name: myrules
  #      rules:
  #      - alert: DatacenterIsOnFire
  #        expr: temperature{cpu} > 100
  #        for: 5m

  # If you prefer to manage your recording/alerting rules in your
  # own ConfigMaps or Secrets, you can use this section to mount
  # those into the Prometheus pods. Remember to extend the `ruleFiles`
  # section above to have your files be loaded into Prometheus.
  # For each volume, specify either a configMap name or a secretName,
  # never both.
  #volumes:
  #- name: initech-alerting-rules
  #  mountPath: /initech/alerts
  #  configMap: initech-alerting-rules-configmap
  #- name: initech-recording-rules
  #  mountPath: /initech/recordings
  #  secretName: initech-recording-rules-secret

  # For larger deployments it can make sense to increase the CPU/memory
  # limits accordingly.
  containers:
    prometheus:
      resources:
        limits:
          cpu: 1
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 512Mi
    reloader:
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 25m
          memory: 16Mi

