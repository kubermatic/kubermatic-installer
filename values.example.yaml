# ====== kubermatic ======
kubermatic:
  docker:
    # the base64 encoded docker authentication token
    secret: ""
  quay:
    # the base64 encoded quay.io authentication token
    secret: ""
  auth:
    # the full path to the openid connect token issuer. For example 'https://dev.kubermatic.io/dex'
    tokenIssuer: ""
    # the client id for openid connect
    clientID: ""
    # skip tls verification on the token issuer
    skipTokenIssuerTLSVerify: "false"
  # base64 encoded datacenters.yaml
  datacenters: ""
  # external domain for the kubermatic installation. For example 'dev.kubermatic.io'
  domain: ""
  # base64 encoded kubeconfig which gives admin access to all seed clusters
  kubeconfig: ""
  etcd:
    # PV size for the etcd StatefulSet of new clusters
    diskSize: "5Gi"
  controller:
    datacenterName: ""
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.7.0-rc1"
      pullPolicy: "IfNotPresent"
    addons:
      image:
        repository: "quay.io/kubermatic/addons"
        tag: "v0.1.8"
        pullPolicy: "IfNotPresent"
      # list of Addons to install into every user-cluster. All need to exist in the addons image
      defaultAddons:
      - canal
      - dashboard
      - dns
      - kube-proxy
      - openvpn
      - rbac
      - kubelet-configmap
      - default-storage-class
      - metrics-server
  api:
    replicas: 2
    image:
      repository: "kubermatic/api"
      tag: "v2.7.0-rc1"
      pullPolicy: "IfNotPresent"
  ui:
    replicas: 2
    image:
      repository: "kubermatic/ui-v2"
      tag: "v0.37"
      pullPolicy: "IfNotPresent"
    config: |
      {
        "show_demo_info": false
      }
  s3_exporter:
    image:
      repository: quay.io/kubermatic/s3-exporter
      tag: v0.2
    endpoint: http://minio.minio.svc.cluster.local:9000
    bucket: kubermatic-etcd-backups
  storeContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader store --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --create-bucket --prefix $CLUSTER --file /backup/snapshot.db
      s3-storeuploader delete-old-revisions --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER --file /backup/snapshot.db --max-revisions 20
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: store-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY
    volumeMounts:
    - name: etcd-backup
      mountPath: /backup
  cleanupContainer: |
    command:
    - /bin/sh
    - -c
    - |
      set -euo pipefail
      s3-storeuploader delete-all --endpoint minio.minio.svc.cluster.local:9000 --bucket kubermatic-etcd-backups --prefix $CLUSTER
    image: quay.io/kubermatic/s3-storer:v0.1.4
    name: cleanup-container
    env:
    - name: ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: ACCESS_KEY_ID
    - name: SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-credentials
          key: SECRET_ACCESS_KEY

# ====== cert-manager ======
# Default values for cert-manager.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1

image:
  repository: quay.io/jetstack/cert-manager-controller
  tag: v0.2.3
  pullPolicy: Always

createCustomResource: true

rbac:
  enabled: true

resources: {}

# ====== certs ======
certificates:
  domains:
  - "dev.kubermatic.io"
  - "grafana.dev.kubermatic.io"
  - "kibana.dev.kubermatic.io"
  - "prometheus.dev.kubermatic.io"
  - "alertmanager.dev.kubermatic.io"

# ====== nginx-ingress-controller ======
##### Kubermatic ingress
nginx:
  hostNetwork: false
  asDaemonSet: false
  replicas: 3
  ignoreMasterTaint: false
  prometheus:
    port: 10254
    scrape: true
  image:
    repository: quay.io/kubernetes-ingress-controller/nginx-ingress-controller
    tag: 0.18.0
  defaultBackend:
    image:
      repository: gcr.io/google_containers/defaultbackend
      tag: 1.4
  config: {}
#    load-balance: "least_conn"

# ====== nodeport-proxy ======
nodePortPoxy:
  image:
    repository: "quay.io/kubermatic/nodeport-proxy"
    tag: "v1.2"

# ====== oauth ======
##### Kubermatic oauth
dex:
  image:
    repository: "quay.io/coreos/dex"
    tag: "v2.10.0"
  replicas: 1
  ingress:
    host: "dev.kubermatic.io"
    path: "/dex"
  connectors:
  - id: github
    type: github
    name: GitHub
    config:
      clientID: some-client-id
      clientSecret: some-client-secret
      redirectURI: https://dev.kubermatic.io/dex/callback
      orgs:
      - name: your-org
  clients:
  - id: kubermatic
    name: Kubermatic
    secret: very-secret
    RedirectURIs:
    - https://dev.kubermatic.io
    - https://dev.kubermatic.io/clusters
  - id: grafana
    name: grafana
    secret: grafana_secret
    RedirectURIs:
    - https://grafana.dev.kubermatic.io/oauth/callback
  - id: kibana
    name: kibana
    secret: kibana_secret
    RedirectURIs:
    - https://kibana.dev.kubermatic.io/oauth/callback
  - id: prometheus
    name: prometheus
    secret: prometheus_secret
    RedirectURIs:
    - https://prometheus.dev.kubermatic.io/oauth/callback
  - id: alertmanager
    name: alertmanager
    secret: alertmanager_secret
    RedirectURIs:
    - https://alertmanager.dev.kubermatic.io/oauth/callback

iap:
  deployments:
    grafana:
      name: grafana
      client_id: grafana
      client_secret: grafana_secret
      encryption_key: grafana_enckey
      upstream_service: grafana.monitoring.svc.cluster.local
      upstream_port: 3000
      ingress:
        host: "grafana.dev.kubermatic.io"
      config:
        enable-authorization-header: false
        scopes:
        - "groups"
        resources:
        - uri: "/*"
          groups:
          - "kubermatic"
    kibana:
      name: kibana
      client_id: kibana
      client_secret: kibana_secret
      encryption_key: kibana_enckey
      upstream_service: kibana-logging.logging.svc.cluster.local
      upstream_port: 5601
      ingress:
        host: "kibana.dev.kubermatic.io"
      config:
        scopes:
        - "groups"
        resources:
        - uri: "/*"
          groups:
          - "kubermatic"
    prometheus:
      name: prometheus
      client_id: prometheus
      client_secret: prometheus_secret
      encryption_key: prometheus_enckey
      upstream_service: prometheus-kubermatic.monitoring.svc.cluster.local
      upstream_port: 9090
      ingress:
        host: "prometheus.dev.kubermatic.io"
      config:
        scopes:
        - "groups"
        resources:
        - uri: "/*"
          groups:
          - "kubermatic:dev"
    alertmanager:
      name: alertmanager
      client_id: alertmanager
      client_secret: alertmanager_secret
      encryption_key: alertmanager_enckey
      upstream_service: alertmanager-kubermatic.monitoring.svc.cluster.local
      upstream_port: 9093
      ingress:
        host: "alertmanager.dev.kubermatic.io"
      config:
        scopes:
        - "groups"
        resources:
        - uri: "/*"
          groups:
          - "kubermatic:dev"
  discovery_url: https://dev.kubermatic.io/dex/.well-known/openid-configuration
  port: 3000

#  staticPasswordLogins:
#  - email: "admin@example.com"
#    # bcrypt hash of the string "password"
#    hash: "$2a$10$2b2cU8CPhOTaGrs1HRQuAueS7JTT5ZHsHSzYiFPm1leZck7Mc8T4W"
#    username: "admin"
#    userID: "08a8684b-db88-4b73-90a9-3cd1661f5466"

# ====== minio ======
minio:
  image:
    repository: "minio/minio"
    tag: "RELEASE.2018-06-09T03-43-35Z"
  storeSize: "100Gi"
  credentials:
    accessKey: "wtupllWfpMg414ZM5YkzZiUmgjh1vZdk"
    secretKey: "r89xkN9JvHJQppb5v7SEfkNkiC1vDcMySQFKxg6uDkE3gZfCeB7ZBfECyUOTywym"


# ========================
# ====== Monitoring ======
# ========================

# ====== alertmanager ======
alertmanager:
  version: v0.15.0
  config:
    global:
      slack_api_url: https://
    route:
      receiver: 'default'
      group_by: ['alertname', 'cluster']
      group_wait: 10s
      group_interval: 5m
      repeat_interval: 1h
    receivers:
    - name: 'default'
      slack_configs:
      - channel: '#alerting'
        send_resolved: true

# ====== grafana ======
grafana:
  image:
    repository: quay.io/coreos/monitoring-grafana
    tag: 5.0.0
  watcher:
    image:
      repository: quay.io/coreos/grafana-watcher
      tag: v0.0.8

# ====== kube-state-metrics ======
kubeStateMetrics:
  image:
    repository: quay.io/coreos/kube-state-metrics
    tag: v1.3.1
  resizer:
    image:
      repository: k8s.gcr.io/addon-resizer
      tag: '1.7'

# ====== node-exporter ======
nodeExporter:
  image:
    repository: quay.io/prometheus/node-exporter
    tag: v0.15.2

# ====== prometheus ======
prometheus:
  version: 'v2.2.1'
  storageSize: 100Gi
  externalLabels:
    region: default
  containers:
    prometheus:
      resources:
        limits:
          cpu: 1
          memory: 2Gi
        requests:
          cpu: 100m
          memory: 512Mi
    reloader:
      resources:
        limits:
          cpu: 100m
          memory: 64Mi
        requests:
          cpu: 25m
          memory: 16Mi

